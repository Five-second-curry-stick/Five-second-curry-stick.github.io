<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>DL3 - 加快神经网络训练速度的优化算法 | 个人网站-杨健豪</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Welcome to MyBlog! 本文所以截图以及文字均来自于：Coursera1 小批量梯度下降算法（mini-batch gradient descent) 首先将你的训练集拆分成更小的 微小的训练集 即小批量训练集(mini-batch) 比如说每一个微型训练集只有1000个训练样例 也就是说 取x1至x1000作为第一个微训练集 也叫做小批量训练集 然后取接下来的1000个样例 x10">
<meta property="og:type" content="article">
<meta property="og:title" content="DL3 - 加快神经网络训练速度的优化算法">
<meta property="og:url" content="http://example.com/2019/08/17/DL3%20-%20optimization%20algorithms/index.html">
<meta property="og:site_name" content="个人网站-杨健豪">
<meta property="og:description" content="Welcome to MyBlog! 本文所以截图以及文字均来自于：Coursera1 小批量梯度下降算法（mini-batch gradient descent) 首先将你的训练集拆分成更小的 微小的训练集 即小批量训练集(mini-batch) 比如说每一个微型训练集只有1000个训练样例 也就是说 取x1至x1000作为第一个微训练集 也叫做小批量训练集 然后取接下来的1000个样例 x10">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2019/08/28/57gxIMFENLkJWv9.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/YQXVuR8HgA4lrd3.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/Y1HuTEiq6vUFjPl.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/9IXPFviqufD4YoS.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/akwGWuSgt32xMIj.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/azu2tUcdsJ9qBn7.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/3LOcsl7n4fFihtW.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/cqlDBOVoQ5GRgWa.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/18qR9I4vEFDBTMr.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/tCSQj5FnGyJoNTO.png">
<meta property="og:image" content="https://i.loli.net/2019/08/28/R3zsKiFMrCyvdu2.png">
<meta property="article:published_time" content="2019-08-16T18:40:57.000Z">
<meta property="article:modified_time" content="2019-09-20T01:12:12.000Z">
<meta property="article:author" content="Jianhao Yang">
<meta property="article:tag" content="deeplearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2019/08/28/57gxIMFENLkJWv9.png">
  
    <link rel="alternate" href="/atom.xml" title="个人网站-杨健豪" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">个人网站-杨健豪</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-DL3 - optimization algorithms" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/08/17/DL3%20-%20optimization%20algorithms/" class="article-date">
  <time datetime="2019-08-16T18:40:57.000Z" itemprop="datePublished">2019-08-17</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/deeplearning/">deeplearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      DL3 - 加快神经网络训练速度的优化算法
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://five-second-curry-stick.github.io/">MyBlog</a>!</p>
<h2 id="本文所以截图以及文字均来自于：Coursera"><a href="#本文所以截图以及文字均来自于：Coursera" class="headerlink" title="本文所以截图以及文字均来自于：Coursera"></a>本文所以截图以及文字均来自于：<a target="_blank" rel="noopener" href="https://www.coursera.org/learn/deep-neural-network/home/week/2">Coursera</a></h2><h3 id="1-小批量梯度下降算法（mini-batch-gradient-descent"><a href="#1-小批量梯度下降算法（mini-batch-gradient-descent" class="headerlink" title="1 小批量梯度下降算法（mini-batch gradient descent)"></a>1 小批量梯度下降算法（mini-batch gradient descent)</h3><blockquote>
<p>首先将你的训练集拆分成更小的 微小的训练集 即小批量训练集(mini-batch) 比如说每一个微型训练集只有1000个训练样例 也就是说 取x1至x1000作为第一个微训练集 也叫做小批量训练集 然后取接下来的1000个样例 x1001至x2000这1000个样例 依次继续</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/57gxIMFENLkJWv9.png"></p>
</blockquote>
</blockquote>
<blockquote>
<p>将mini-batch极端的设置为m，就得到了批量梯度下降</p>
</blockquote>
<blockquote>
<p>极端地设置为1，就得到了随机梯度下降</p>
</blockquote>
<blockquote>
<p>两种方法的区别：批量梯度下降算法可能从这里开始 它的噪声相对小些 每一步相对大些 并且最终可以达到最小值 而相对的 随机梯度下降算法 让我们选一个不同的点 假使从这里开始 这时对于每一次迭代你就在一个样本上做梯度下降 大多数时候你可以达到全局最小值 但是有时候也可能因为某组数据不太好 把你指向一个错误的方向 因此随机梯度算法的噪声会非常大 一般来说它会沿着正确的方向 但是有事也会指向错误的方向 而且随机梯度下降算法 最后也不会收敛到一个点 它一般会在最低点附近摆动 但是不会达到并且停在那里 实际上 mini-batch的大小一般会在这2个极端之间</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/YQXVuR8HgA4lrd3.png"></p>
</blockquote>
</blockquote>
<h3 id="2-指数加权（滑动）平均"><a href="#2-指数加权（滑动）平均" class="headerlink" title="2 指数加权（滑动）平均"></a>2 指数加权（滑动）平均</h3><blockquote>
<p>beta*V_(t-1)加上 之前使用的是0.1 现在把它换成(1-beta)*theta_t 之前beta=0.9 出于我们之后会讲的某些原因 当你计算这个公式的时候 你可以认为V_t近似于 1/(1-beta)天温度的平均 举例来说 当beta=0.9的时候 你可以认为它是前10天的气温平均值</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/Y1HuTEiq6vUFjPl.png"></p>
</blockquote>
</blockquote>
<h3 id="3-偏差修正"><a href="#3-偏差修正" class="headerlink" title="3 偏差修正"></a>3 偏差修正</h3><blockquote>
<p>它能够帮助你更好地计算平均值</p>
</blockquote>
<blockquote>
<p>工作原理：用vt/1-βt代替vt（t是下标）</p>
</blockquote>
<blockquote>
<p>在机器学习中 多数的指数加权平均运算 并不会使用偏差修正 因为大多数人更愿意在初始阶段 用一个稍带偏差的值进行运算 不过 如果在初始阶段就开始考虑偏差 指数加权移动均指仍处于预热阶段 偏差修正可以帮你尽早做出更好的估计</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/9IXPFviqufD4YoS.png"></p>
</blockquote>
</blockquote>
<h3 id="4-动量梯度下降算法"><a href="#4-动量梯度下降算法" class="headerlink" title="4 动量梯度下降算法"></a>4 动量梯度下降算法</h3><blockquote>
<p>它几乎总会比标准的梯度下降算法更快 一言以蔽之 算法的主要思想是 计算梯度的指数加权平均 然后使用这个梯度来更新权重</p>
</blockquote>
<blockquote>
<p>可以减少震荡，原因：如果把这些梯度平均一下 你会发现这些震荡 在纵轴上的平均值趋近于0 所以 在垂直方向上 你会希望减慢速度 正数和负数在计算平均时相互抵消了 平均值接近于0 然而在水平方向上 所有导数都指向水平方向的右边 所以水平方向的平均值仍然较大 因此在数次迭代之后 你会发现动量梯度下降算法的每一步 在垂直方向上的振荡非常小 且在水平方向上运动得更快 这会让你的算法选择更加直接的路径 或者说减弱了前往最小值的路径上的振荡</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/akwGWuSgt32xMIj.png"></p>
</blockquote>
</blockquote>
<h3 id="5-RMSprop-均方根传递（root-mean-square-prop）"><a href="#5-RMSprop-均方根传递（root-mean-square-prop）" class="headerlink" title="5 RMSprop 均方根传递（root mean square prop）"></a>5 RMSprop 均方根传递（root mean square prop）</h3><blockquote>
<p>你希望减慢b方向的学习 也就是垂直方向 同时加速或至少不减慢水平方向的学习 这就是RMSprop算法要做的</p>
</blockquote>
<blockquote>
<p>现在我们来理解一下它的工作原理 记得在水平方向上 即例子中W的方向上 我们希望学习速率较快 而在垂直方向上 即例子中b的方向上 我们希望降低垂直方向上的振荡 对于S_dW和S_db这两项 我们希望S_dW相对较小 因此这里除以的是一个较小的数 而S_db相对较大 因此这里除以的是一个较大的数 这样就可以减缓垂直方向上的更新</p>
</blockquote>
<blockquote>
<p>另一个收效是 你可以使用更大的学习率alpha 学习得更快 而不用担心在垂直方向上发散 </p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/azu2tUcdsJ9qBn7.png"></p>
</blockquote>
</blockquote>
<h3 id="6-Adam优化算法（自适应矩估计Adaptive-Moment-Estimation）"><a href="#6-Adam优化算法（自适应矩估计Adaptive-Moment-Estimation）" class="headerlink" title="6 Adam优化算法（自适应矩估计Adaptive Moment Estimation）"></a>6 Adam优化算法（自适应矩估计Adaptive Moment Estimation）</h3><blockquote>
<p>Adam优化算法本质上是将 动量算法和RMSprop结合起来:在动量梯度下降算法抵消部分震荡的前提下，利用了rms梯度下降算法降低震荡</p>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/3LOcsl7n4fFihtW.png"><br>t表示迭代次数</p>
</blockquote>
<p>超参数的选择</p>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/cqlDBOVoQ5GRgWa.png"></p>
</blockquote>
</blockquote>
<h3 id="7-学习率衰减-learning-rate-decay"><a href="#7-学习率衰减-learning-rate-decay" class="headerlink" title="7 学习率衰减 learning rate decay"></a>7 学习率衰减 learning rate decay</h3><blockquote>
<p>如果你想使用学习率衰减 你可以尝试 不同的超参数组合 包括α0 以及这个衰减率的超参数 然后去尝试寻找一个效果好的数值</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/18qR9I4vEFDBTMr.png"></p>
</blockquote>
</blockquote>
<h4 id="7-1-其他学习率衰减的方法"><a href="#7-1-其他学习率衰减的方法" class="headerlink" title="7.1 其他学习率衰减的方法"></a>7.1 其他学习率衰减的方法</h4><blockquote>
<p>k表示常数</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/tCSQj5FnGyJoNTO.png"></p>
</blockquote>
</blockquote>
<h3 id="8-局部最优点，鞍点"><a href="#8-局部最优点，鞍点" class="headerlink" title="8 局部最优点，鞍点"></a>8 局部最优点，鞍点</h3><blockquote>
<p>对于一个高维空间的函数 如果梯度为零 则在每个方向上 它可能是凸函数 或者是凹函数 假设在一个 2万维的空间中 如果一个点要成为局部最优 则需要在所有的2万个方向上都像这样 因此这件事发生的概率非常低 大概2的负2万次方 你更有可能遇到的情况是<br />某些方向的曲线像这样向上弯曲 同时另一些方向的曲线则向下弯曲 并非所有曲线都向上弯曲 <strong><em>这就是为什么在高维空间中 你更有可能碰到一个像右图这样的鞍点 而不是局部最优</em></strong></p>
</blockquote>
<h4 id="8-1-停滞区"><a href="#8-1-停滞区" class="headerlink" title="8.1 停滞区"></a>8.1 停滞区</h4><blockquote>
<p>实际上是停滞区(Plateaus) 停滞区指的是 导数长时间接近于零的一段区域 如果你在这里 那么梯度下降会沿着这个曲面向下移动 然而因为梯度为零或接近于零 曲面很平 你会花费很长的时间 缓慢地在停滞区里找到这个点 然后因为左侧或右侧的随机扰动,你的算法终于能够离开这个停滞区 它一直沿着这个很长的坡往下走, 直到抵达此处, 离开这个停滞区</p>
</blockquote>
<blockquote>
<blockquote>
<p><img src="https://i.loli.net/2019/08/28/R3zsKiFMrCyvdu2.png"></p>
</blockquote>
</blockquote>
<blockquote>
<p>首先 实际上你不太可能陷入糟糕的局部最优点 只要你训练的是一个较大的神经网络 有很多参数 代价函数J定义在一个相对高维的空间上 </p>
</blockquote>
<blockquote>
<p>其次 停滞区是个问题, 它会让学习过程变得相当慢 这也是像动量(Momentum)算法<br />或RmsProp算法 或Adam算法能改善你的学习算法的地方</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/08/17/DL3%20-%20optimization%20algorithms/" data-id="ckgqfij2f000a7545167v4va7" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning/" rel="tag">deeplearning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/08/19/DL4%20-%20Hyperparemeter%20tuning/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          DL4 - 超参数的调整、批量标准化
        
      </div>
    </a>
  
  
    <a href="/2019/08/14/DL2%20-%20Improving%20Deep%20Neural%20Networks-%20Hyperparameter%20tuning,%20Regularization%20and%20Optimization/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">DL2 - 在实际应用中如何使得神经网络高效工作</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine-Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/deeplearning/">deeplearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/">数值优化</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/">数学之美</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%96%87%E7%8C%AE%E7%90%86%E8%A7%A3/">文献理解</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/cousera/" rel="tag">cousera</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deeplearning/" rel="tag">deeplearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/" rel="tag">数值优化</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/" rel="tag">数学之美</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%96%87%E7%8C%AE%E7%90%86%E8%A7%A3/" rel="tag">文献理解</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/cousera/" style="font-size: 15px;">cousera</a> <a href="/tags/deeplearning/" style="font-size: 20px;">deeplearning</a> <a href="/tags/python/" style="font-size: 12.5px;">python</a> <a href="/tags/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96/" style="font-size: 12.5px;">数值优化</a> <a href="/tags/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E/" style="font-size: 17.5px;">数学之美</a> <a href="/tags/%E6%96%87%E7%8C%AE%E7%90%86%E8%A7%A3/" style="font-size: 10px;">文献理解</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">八月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">四月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/10/09/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%964%20-%20%E7%BA%BF%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95%E7%9A%84%E6%94%B6%E6%95%9B%E6%80%A7%E5%92%8C%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/10/05/%E4%B8%8D%E5%AE%8C%E6%95%B4/%E6%96%87%E7%8C%AE%E7%90%86%E8%A7%A31%20-%20LCF/">文献理解 - LCF</a>
          </li>
        
          <li>
            <a href="/2019/09/26/%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E16%20-%20%E4%BF%A1%E6%81%AF%E6%8C%87%E7%BA%B9/">数学之美16 - 信息指纹及其应用</a>
          </li>
        
          <li>
            <a href="/2019/09/25/DL12%20-%20CNN%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">DL12 - CNN基础知识</a>
          </li>
        
          <li>
            <a href="/2019/09/22/%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%963%20-%20%E7%BA%BF%E6%80%A7%E6%90%9C%E7%B4%A2%E6%96%B9%E6%B3%95/">数值优化3 - 线搜索方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Jianhao Yang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>